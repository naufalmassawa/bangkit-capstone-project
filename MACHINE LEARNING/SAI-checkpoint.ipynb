{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FLyRE4NZkdPH"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6bPRwve-O2D"
   },
   "source": [
    "# buat direktori train dan test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vFocedjemBj0"
   },
   "outputs": [],
   "source": [
    "root_dir = 'data'\n",
    "\n",
    "if os.path.exists(root_dir):\n",
    "  shutil.rmtree(root_dir)\n",
    "\n",
    "def create_dir (root_path):\n",
    "  kelas = ['Andreas', 'Boy Satria', 'Channel', 'Evan', 'Ivan', 'Laura', 'Mariana', 'Martinus', 'Winner', 'Fedrick']\n",
    "\n",
    "  for cl_name in kelas :\n",
    "    os.makedirs(os.path.join(f'{root_path}/training', cl_name))\n",
    "    os.makedirs(os.path.join(f'{root_path}/testing', cl_name))\n",
    "\n",
    "try :\n",
    "  create_dir (root_path=root_dir)\n",
    "except FileExistsError:\n",
    "  print(\"file doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gN5eJVumacq",
    "outputId": "83dd3c14-494d-4f84-eb9c-a1cd82ba6905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\testing\n",
      "data\\training\n",
      "data\\testing\\Andreas\n",
      "data\\testing\\Boy Satria\n",
      "data\\testing\\Channel\n",
      "data\\testing\\Evan\n",
      "data\\testing\\Fedrick\n",
      "data\\testing\\Ivan\n",
      "data\\testing\\Laura\n",
      "data\\testing\\Mariana\n",
      "data\\testing\\Martinus\n",
      "data\\testing\\Winner\n",
      "data\\training\\Andreas\n",
      "data\\training\\Boy Satria\n",
      "data\\training\\Channel\n",
      "data\\training\\Evan\n",
      "data\\training\\Fedrick\n",
      "data\\training\\Ivan\n",
      "data\\training\\Laura\n",
      "data\\training\\Mariana\n",
      "data\\training\\Martinus\n",
      "data\\training\\Winner\n"
     ]
    }
   ],
   "source": [
    "for rootdir, dirs, files in os.walk(root_dir):\n",
    "  for subdir in dirs :\n",
    "    print(os.path.join(rootdir, subdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCj2Nt_x-ZYM"
   },
   "source": [
    "# menghubungkan direktori dataset ke training set dan test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y_cmqPKu7u2A"
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    files = []\n",
    "    for filename in os.listdir(SOURCE):\n",
    "        file = SOURCE + filename\n",
    "        if os.path.getsize(file) > 0:\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            print(filename + \" is zero length, so ignoring.\")\n",
    "            \n",
    "    files = os.listdir(SOURCE)\n",
    "    files = [file for file in files if os.path.getsize(os.path.join(SOURCE, file))>0]\n",
    "    files = random.sample(files, len(files))\n",
    "    limit = int(len(files) * SPLIT_SIZE)\n",
    "\n",
    "    for index, file in enumerate(files):\n",
    "        source = os.path.join(SOURCE, file)\n",
    "        if index < limit:\n",
    "            destination = os.path.join(TRAINING, file)\n",
    "        else:\n",
    "            destination = os.path.join(TESTING, file)  \n",
    "    \n",
    "        copyfile(source, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDRc35Cq-i0l"
   },
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "WYXTvcG9-mGw",
    "outputId": "96cb08b9-147d-4e89-e7cf-6b8b2af6b3f0"
   },
   "outputs": [],
   "source": [
    "andreas_dir = \"dataset/Andreas/\"\n",
    "boy_dir = \"dataset/Boy Satria/\"\n",
    "channel_dir = \"dataset/Channel/\"\n",
    "evan_dir = \"dataset/Evan/\"\n",
    "fedrick_dir = \"dataset/Fedrick/\"\n",
    "ivan_dir = \"dataset/Ivan/\"\n",
    "laura_dir = \"dataset/Laura/\"\n",
    "mariana_dir = \"dataset/Mariana/\"\n",
    "martinus_dir = \"dataset/Martinus/\"\n",
    "winner_dir = \"dataset/Winner/\"\n",
    "\n",
    "train_dir = \"data/training/\"\n",
    "test_dir = \"data/testing/\"\n",
    "\n",
    "training_andreas_dir = os.path.join(train_dir, \"Andreas/\")\n",
    "training_boy_dir = os.path.join(train_dir, \"Boy Satria/\")\n",
    "training_channel_dir = os.path.join(train_dir, \"Channel/\")\n",
    "training_evan_dir = os.path.join(train_dir, \"Evan/\")\n",
    "training_fedrick_dir = os.path.join(train_dir, \"Fedrick/\")\n",
    "training_ivan_dir = os.path.join(train_dir, \"Ivan/\")\n",
    "training_laura_dir = os.path.join(train_dir, \"Laura/\")\n",
    "training_mariana_dir = os.path.join(train_dir, \"Mariana/\")\n",
    "training_martinus_dir = os.path.join(train_dir, \"Martinus/\")\n",
    "training_winner_dir = os.path.join(train_dir, \"Winner/\")\n",
    "\n",
    "testing_andreas_dir = os.path.join(test_dir, \"Andreas/\")\n",
    "testing_boy_dir = os.path.join(test_dir, \"Boy Satria/\")\n",
    "testing_channel_dir = os.path.join(test_dir, \"Channel/\")\n",
    "testing_evan_dir = os.path.join(test_dir, \"Evan/\")\n",
    "testing_fedrick_dir = os.path.join(test_dir, \"Fedrick/\")\n",
    "testing_ivan_dir = os.path.join(test_dir, \"Ivan/\")\n",
    "testing_laura_dir = os.path.join(test_dir, \"Laura/\")\n",
    "testing_mariana_dir = os.path.join(test_dir, \"Mariana/\")\n",
    "testing_martinus_dir = os.path.join(test_dir, \"Martinus/\")\n",
    "testing_winner_dir = os.path.join(test_dir, \"Winner/\")\n",
    "\n",
    "if len(os.listdir(training_andreas_dir)) > 0:\n",
    "  for f in os.scandir(training_andreas_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(training_boy_dir)) > 0:\n",
    "  for f in os.scandir(training_boy_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(training_channel_dir)) > 0:\n",
    "  for f in os.scandir(training_channel_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(training_evan_dir)) > 0:\n",
    "  for f in os.scandir(training_evan_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(training_fedrick_dir)) > 0:\n",
    "  for f in os.scandir(training_fedrick_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(training_ivan_dir)) > 0:\n",
    "  for f in os.scandir(training_ivan_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(training_laura_dir)) > 0:\n",
    "  for f in os.scandir(training_laura_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(training_mariana_dir)) > 0:\n",
    "  for f in os.scandir(training_mariana_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(training_martinus_dir)) > 0:\n",
    "  for f in os.scandir(training_martinus_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(training_winner_dir)) > 0:\n",
    "  for f in os.scandir(training_winner_dir):\n",
    "    os.remove(f.path)\n",
    "\n",
    "if len(os.listdir(testing_andreas_dir)) > 0:\n",
    "  for f in os.scandir(testing_andreas_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(testing_boy_dir)) > 0:\n",
    "  for f in os.scandir(testing_boy_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(testing_channel_dir)) > 0:\n",
    "  for f in os.scandir(testing_channel_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(testing_evan_dir)) > 0:\n",
    "  for f in os.scandir(testing_evan_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(testing_fedrick_dir)) > 0:\n",
    "  for f in os.scandir(testing_fedrick_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(testing_ivan_dir)) > 0:\n",
    "  for f in os.scandir(testing_ivan_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(testing_laura_dir)) > 0:\n",
    "  for f in os.scandir(testing_laura_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(testing_mariana_dir)) > 0:\n",
    "  for f in os.scandir(testing_mariana_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(testing_martinus_dir)) > 0:\n",
    "  for f in os.scandir(testing_martinus_dir):\n",
    "    os.remove(f.path)\n",
    "if len(os.listdir(testing_winner_dir)) > 0:\n",
    "  for f in os.scandir(testing_winner_dir):\n",
    "    os.remove(f.path)\n",
    "\n",
    "split_size = .9\n",
    "\n",
    "split_data(andreas_dir, training_andreas_dir, testing_andreas_dir, split_size)\n",
    "split_data(boy_dir, training_boy_dir, testing_boy_dir, split_size)\n",
    "split_data(channel_dir, training_channel_dir, testing_channel_dir, split_size)\n",
    "split_data(evan_dir, training_evan_dir, testing_evan_dir, split_size)\n",
    "split_data(fedrick_dir, training_fedrick_dir, testing_fedrick_dir, split_size)\n",
    "split_data(ivan_dir, training_ivan_dir, testing_ivan_dir, split_size)\n",
    "split_data(laura_dir, training_laura_dir, testing_laura_dir, split_size)\n",
    "split_data(mariana_dir, training_mariana_dir, testing_mariana_dir, split_size)\n",
    "split_data(martinus_dir, training_martinus_dir, testing_martinus_dir, split_size)\n",
    "split_data(winner_dir, training_winner_dir, testing_winner_dir, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5S0VoGj-9_U4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def train_val_gen (train_dir, test_dir):\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.,\n",
    "                                       rotation_range=40,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       horizontal_flip=True)\n",
    "    \n",
    "    train_gen = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                  batch_size = 10,\n",
    "                                                  class_mode = 'binary',\n",
    "                                                  color_mode = 'grayscale',\n",
    "                                                  target_size = (50,50))\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "    val_gen = val_datagen.flow_from_directory(directory=test_dir,\n",
    "                                              batch_size = 2,\n",
    "                                              class_mode = 'binary',\n",
    "                                              color_mode = 'grayscale',\n",
    "                                              target_size = (50,50))\n",
    "    \n",
    "    return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 10 classes.\n",
      "Found 100 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen, val_gen = train_val_gen(train_dir, test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "def create_model ():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(8, (3,3), activation = 'relu', padding='same', input_shape=(50,50,1)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer = Adam (lr=(0.001)),\n",
    "                  loss = 'sparse_categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 50, 50, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 25, 25, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 25, 25, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 13, 13, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              132096    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 762,586\n",
      "Trainable params: 762,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 2.3060 - acc: 0.0865Epoch 1/150\n",
      "90/90 [==============================] - 4s 40ms/step - loss: 2.3060 - acc: 0.0856 - val_loss: 2.3023 - val_acc: 0.1000\n",
      "Epoch 2/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 2.2231 - acc: 0.1500- ETA: 1s - loss: 2Epoch 1/150\n",
      "90/90 [==============================] - 3s 36ms/step - loss: 2.2199 - acc: 0.1522 - val_loss: 1.9112 - val_acc: 0.2000\n",
      "Epoch 3/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 1.7880 - acc: 0.2724Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 1.7806 - acc: 0.2744 - val_loss: 1.2762 - val_acc: 0.3800\n",
      "Epoch 4/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 1.4057 - acc: 0.4607Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 1.4020 - acc: 0.4633 - val_loss: 0.6639 - val_acc: 0.7300\n",
      "Epoch 5/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 1.1769 - acc: 0.5782Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 1.1737 - acc: 0.5756 - val_loss: 0.4200 - val_acc: 0.8600\n",
      "Epoch 6/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.8564 - acc: 0.7146Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.8594 - acc: 0.7144 - val_loss: 0.3504 - val_acc: 0.9200\n",
      "Epoch 7/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.7586Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.6395 - acc: 0.7622 - val_loss: 0.4126 - val_acc: 0.8600\n",
      "Epoch 8/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8239Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.5231 - acc: 0.8244 - val_loss: 0.2171 - val_acc: 0.9500\n",
      "Epoch 9/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.4415 - acc: 0.8534- ETA: 1s -Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.4388 - acc: 0.8544 - val_loss: 0.1710 - val_acc: 0.9500\n",
      "Epoch 10/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8545Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.4159 - acc: 0.8556 - val_loss: 0.2149 - val_acc: 0.9400\n",
      "Epoch 11/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.8591Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.4384 - acc: 0.8611 - val_loss: 0.1909 - val_acc: 0.9600\n",
      "Epoch 12/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.3795 - acc: 0.8708Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.3827 - acc: 0.8711 - val_loss: 0.1279 - val_acc: 0.9600\n",
      "Epoch 13/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.8989Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.2979 - acc: 0.9000 - val_loss: 0.1817 - val_acc: 0.9600\n",
      "Epoch 14/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9034Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.2586 - acc: 0.9044 - val_loss: 0.1412 - val_acc: 0.9600\n",
      "Epoch 15/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9368Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.2269 - acc: 0.9367 - val_loss: 0.1824 - val_acc: 0.9600\n",
      "Epoch 16/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9000Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.2942 - acc: 0.8956 - val_loss: 0.2103 - val_acc: 0.9400\n",
      "Epoch 17/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.9102Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.2854 - acc: 0.9089 - val_loss: 0.1997 - val_acc: 0.9600\n",
      "Epoch 18/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9046Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.2908 - acc: 0.9067 - val_loss: 0.1160 - val_acc: 0.9600\n",
      "Epoch 19/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9448Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.1925 - acc: 0.9433 - val_loss: 0.2040 - val_acc: 0.9500\n",
      "Epoch 20/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9303Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.2132 - acc: 0.9289 - val_loss: 0.1849 - val_acc: 0.9600\n",
      "Epoch 21/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9250Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.2409 - acc: 0.9244 - val_loss: 0.1525 - val_acc: 0.9600\n",
      "Epoch 22/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9466- ETA: 2s - loss: 0. - ETA: 0s - loss: 0.169Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1752 - acc: 0.9467 - val_loss: 0.1772 - val_acc: 0.9600\n",
      "Epoch 23/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9438Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.1825 - acc: 0.9444 - val_loss: 0.1423 - val_acc: 0.9600\n",
      "Epoch 24/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9494- ETA: 0s - loss: 0.1710 - acc: 0.9Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1755 - acc: 0.9500 - val_loss: 0.1302 - val_acc: 0.9600\n",
      "Epoch 25/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9523Epoch 1/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 3s 32ms/step - loss: 0.1572 - acc: 0.9533 - val_loss: 0.1133 - val_acc: 0.9600\n",
      "Epoch 26/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9414Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1777 - acc: 0.9411 - val_loss: 0.1446 - val_acc: 0.9600\n",
      "Epoch 27/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9409Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.2079 - acc: 0.9400 - val_loss: 0.1810 - val_acc: 0.9500\n",
      "Epoch 28/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9284- ETA: 0s - loss: 0.2449 - accEpoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.2175 - acc: 0.9300 - val_loss: 0.1398 - val_acc: 0.9600\n",
      "Epoch 29/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9494Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.1765 - acc: 0.9456 - val_loss: 0.1261 - val_acc: 0.9800\n",
      "Epoch 30/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9591Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.1381 - acc: 0.9578 - val_loss: 0.1167 - val_acc: 0.9600\n",
      "Epoch 31/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9644- ETA: 1s -Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.1097 - acc: 0.9644 - val_loss: 0.1060 - val_acc: 0.9800\n",
      "Epoch 32/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9460Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.1546 - acc: 0.9478 - val_loss: 0.1480 - val_acc: 0.9600\n",
      "Epoch 33/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9557Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1361 - acc: 0.9544 - val_loss: 0.1144 - val_acc: 0.9700\n",
      "Epoch 34/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9398Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.1620 - acc: 0.9411 - val_loss: 0.1207 - val_acc: 0.9700\n",
      "Epoch 35/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9557- ETA: 1s - loss: Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1343 - acc: 0.9556 - val_loss: 0.1105 - val_acc: 0.9700\n",
      "Epoch 36/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9557Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1428 - acc: 0.9556 - val_loss: 0.1117 - val_acc: 0.9600\n",
      "Epoch 37/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9557Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.1476 - acc: 0.9567 - val_loss: 0.0217 - val_acc: 0.9900\n",
      "Epoch 38/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9670Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.1127 - acc: 0.9656 - val_loss: 0.0924 - val_acc: 0.9600\n",
      "Epoch 39/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9598Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1227 - acc: 0.9578 - val_loss: 0.1880 - val_acc: 0.9600\n",
      "Epoch 40/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9432Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.2116 - acc: 0.9389 - val_loss: 0.0314 - val_acc: 0.9800\n",
      "Epoch 41/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9563Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1274 - acc: 0.9567 - val_loss: 0.0847 - val_acc: 0.9800\n",
      "Epoch 42/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9759Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0770 - acc: 0.9744 - val_loss: 0.0987 - val_acc: 0.9700\n",
      "Epoch 43/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9730Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.0967 - acc: 0.9722 - val_loss: 0.0711 - val_acc: 0.9700\n",
      "Epoch 44/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9539Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1758 - acc: 0.9533 - val_loss: 0.0820 - val_acc: 0.9700\n",
      "Epoch 45/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9652Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1162 - acc: 0.9656 - val_loss: 0.0925 - val_acc: 0.9800\n",
      "Epoch 46/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9563- ETA: 1s - losEpoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1419 - acc: 0.9567 - val_loss: 0.0611 - val_acc: 0.9700\n",
      "Epoch 47/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9563Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1427 - acc: 0.9567 - val_loss: 0.0844 - val_acc: 0.9800\n",
      "Epoch 48/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9607Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1217 - acc: 0.9611 - val_loss: 0.0621 - val_acc: 0.9800\n",
      "Epoch 49/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9830Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0663 - acc: 0.9833 - val_loss: 0.0513 - val_acc: 0.9800\n",
      "Epoch 50/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9888Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0506 - acc: 0.9889 - val_loss: 0.1049 - val_acc: 0.9800\n",
      "Epoch 51/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9818- ETA: 1s - loss: Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.0617 - acc: 0.9811 - val_loss: 0.0887 - val_acc: 0.9700\n",
      "Epoch 52/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9528Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1364 - acc: 0.9533 - val_loss: 0.0795 - val_acc: 0.9800\n",
      "Epoch 53/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9716- ETAEpoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.0891 - acc: 0.9711 - val_loss: 0.0875 - val_acc: 0.9700\n",
      "Epoch 54/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9663Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1001 - acc: 0.9656 - val_loss: 0.0508 - val_acc: 0.9800\n",
      "Epoch 55/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9753Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0643 - acc: 0.9756 - val_loss: 0.0476 - val_acc: 0.9900\n",
      "Epoch 56/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9636Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.1310 - acc: 0.9611 - val_loss: 0.1154 - val_acc: 0.9800\n",
      "Epoch 57/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9655Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.1242 - acc: 0.9667 - val_loss: 0.0697 - val_acc: 0.9800\n",
      "Epoch 58/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9816Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0800 - acc: 0.9822 - val_loss: 0.0800 - val_acc: 0.9800\n",
      "Epoch 59/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9557Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.1486 - acc: 0.9556 - val_loss: 0.0478 - val_acc: 0.9700\n",
      "Epoch 60/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9663Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.1033 - acc: 0.9667 - val_loss: 0.0722 - val_acc: 0.9800\n",
      "Epoch 61/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9705- ETA: 0s - loss: 0.0879 - acc: 0.969Epoch 1/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0842 - acc: 0.9711 - val_loss: 0.0782 - val_acc: 0.9800\n",
      "Epoch 62/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9708Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1002 - acc: 0.9700 - val_loss: 0.0351 - val_acc: 0.9900\n",
      "Epoch 63/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9730Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0790 - acc: 0.9733 - val_loss: 0.0506 - val_acc: 0.9800\n",
      "Epoch 64/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9727Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0770 - acc: 0.9733 - val_loss: 0.0784 - val_acc: 0.9800\n",
      "Epoch 65/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9557Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.1436 - acc: 0.9556 - val_loss: 0.0111 - val_acc: 0.9900\n",
      "Epoch 66/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9851Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0462 - acc: 0.9844 - val_loss: 0.0197 - val_acc: 0.9900\n",
      "Epoch 67/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9795Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0740 - acc: 0.9767 - val_loss: 0.0798 - val_acc: 0.9700\n",
      "Epoch 68/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9685Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.1111 - acc: 0.9689 - val_loss: 0.0406 - val_acc: 0.9800\n",
      "Epoch 69/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9775Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0710 - acc: 0.9767 - val_loss: 0.0907 - val_acc: 0.9700\n",
      "Epoch 70/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9764Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0646 - acc: 0.9767 - val_loss: 0.0675 - val_acc: 0.9800\n",
      "Epoch 71/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9770Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0809 - acc: 0.9778 - val_loss: 0.0222 - val_acc: 0.9900\n",
      "Epoch 72/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9719Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0896 - acc: 0.9722 - val_loss: 0.0643 - val_acc: 0.9800\n",
      "Epoch 73/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9775Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0854 - acc: 0.9778 - val_loss: 0.0312 - val_acc: 0.9900\n",
      "Epoch 74/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9770Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0793 - acc: 0.9767 - val_loss: 0.0646 - val_acc: 0.9900\n",
      "Epoch 75/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9852Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0523 - acc: 0.9856 - val_loss: 0.1019 - val_acc: 0.9800\n",
      "Epoch 76/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9761Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0825 - acc: 0.9756 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 77/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9775Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0742 - acc: 0.9778 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 78/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9584Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.1168 - acc: 0.9589 - val_loss: 0.0270 - val_acc: 0.9900\n",
      "Epoch 79/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9764Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.0583 - acc: 0.9767 - val_loss: 0.0707 - val_acc: 0.9800\n",
      "Epoch 80/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9816- ETA: 1s - loss: 0.0Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0641 - acc: 0.9822 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "Epoch 81/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9910Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0364 - acc: 0.9911 - val_loss: 0.0216 - val_acc: 1.0000\n",
      "Epoch 82/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9701Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0957 - acc: 0.9711 - val_loss: 0.0163 - val_acc: 0.9900\n",
      "Epoch 83/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9761Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0768 - acc: 0.9767 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 84/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9886Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0428 - acc: 0.9867 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "Epoch 85/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9739Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0809 - acc: 0.9744 - val_loss: 0.0356 - val_acc: 0.9900\n",
      "Epoch 86/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9795Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0292 - val_acc: 0.9900\n",
      "Epoch 87/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9820Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.0628 - acc: 0.9822 - val_loss: 0.0299 - val_acc: 0.9900\n",
      "Epoch 88/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9682Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0899 - acc: 0.9689 - val_loss: 0.0623 - val_acc: 0.9700\n",
      "Epoch 89/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9705Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0922 - acc: 0.9711 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 90/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9784Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0843 - acc: 0.9789 - val_loss: 0.0070 - val_acc: 1.0000\n",
      "Epoch 91/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9831Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0585 - acc: 0.9833 - val_loss: 0.0843 - val_acc: 0.9800\n",
      "Epoch 92/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9775Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.0653 - acc: 0.9778 - val_loss: 0.0235 - val_acc: 0.9900\n",
      "Epoch 93/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9864Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0428 - acc: 0.9867 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 94/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9640Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.1750 - acc: 0.9644 - val_loss: 0.0458 - val_acc: 0.9800\n",
      "Epoch 95/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9708Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.1083 - acc: 0.9700 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 96/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9716Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.1080 - acc: 0.9711 - val_loss: 0.0735 - val_acc: 0.9600\n",
      "Epoch 97/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9705Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.0947 - acc: 0.9711 - val_loss: 0.0236 - val_acc: 0.9800\n",
      "Epoch 98/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/90 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9784Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0765 - acc: 0.9789 - val_loss: 0.0152 - val_acc: 0.9900\n",
      "Epoch 99/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9865- ETA: Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0424 - acc: 0.9867 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "Epoch 100/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9920Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0280 - acc: 0.9922 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 101/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9795Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0779 - acc: 0.9789 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 102/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9885Epoch 1/150\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.0000    - 3s 31ms/step - loss: 0.0330 - acc: 0.9889 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 103/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9886Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0352 - acc: 0.9878 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 104/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9921Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0233 - acc: 0.9911 - val_loss: 0.0635 - val_acc: 0.9900\n",
      "Epoch 105/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9830Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0692 - acc: 0.9822 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 106/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9795Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0587 - acc: 0.9800 - val_loss: 0.0287 - val_acc: 0.9900\n",
      "Epoch 107/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9724Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1204 - acc: 0.9733 - val_loss: 0.0368 - val_acc: 0.9900\n",
      "Epoch 108/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9773Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0805 - acc: 0.9778 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 109/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9908Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0335 - acc: 0.9911 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 110/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9784Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0723 - acc: 0.9778 - val_loss: 0.0101 - val_acc: 1.0000\n",
      "Epoch 111/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9899Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.0326 - acc: 0.9900 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 112/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9773Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1128 - acc: 0.9744 - val_loss: 0.0238 - val_acc: 1.0000\n",
      "Epoch 113/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9818Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0804 - acc: 0.9822 - val_loss: 0.0292 - val_acc: 0.9900\n",
      "Epoch 114/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9876Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0532 - acc: 0.9878 - val_loss: 0.0314 - val_acc: 0.9900\n",
      "Epoch 115/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9820Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.0630 - acc: 0.9822 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 116/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9864Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0458 - acc: 0.9867 - val_loss: 0.0168 - val_acc: 0.9900\n",
      "Epoch 117/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9793Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0691 - acc: 0.9789 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 118/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9670- ETEpoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.1402 - acc: 0.9678 - val_loss: 0.0132 - val_acc: 1.0000\n",
      "Epoch 119/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9809Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0686 - acc: 0.9811 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 120/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9841Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0533 - acc: 0.9844 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 121/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9805Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0724 - acc: 0.9811 - val_loss: 0.0212 - val_acc: 0.9900\n",
      "Epoch 122/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9795Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0868 - acc: 0.9778 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 123/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9932Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0214 - acc: 0.9933 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 124/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9831Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0625 - acc: 0.9833 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 125/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9865Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0349 - acc: 0.9856 - val_loss: 6.7367e-04 - val_acc: 1.0000\n",
      "Epoch 126/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9831Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0398 - acc: 0.9833 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 127/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9852Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0496 - acc: 0.9856 - val_loss: 0.0165 - val_acc: 0.9900\n",
      "Epoch 128/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9920Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0167 - acc: 0.9922 - val_loss: 8.2037e-04 - val_acc: 1.0000\n",
      "Epoch 129/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9908Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0268 - acc: 0.9911 - val_loss: 3.2451e-04 - val_acc: 1.0000\n",
      "Epoch 130/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9831Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0501 - acc: 0.9833 - val_loss: 0.1265 - val_acc: 0.9600\n",
      "Epoch 131/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9787Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0760 - acc: 0.9789 - val_loss: 7.1527e-04 - val_acc: 1.0000\n",
      "Epoch 132/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.991 - ETA: 0s - loss: 0.0387 - acc: 0.9910Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0516 - acc: 0.9889 - val_loss: 0.0191 - val_acc: 0.9900\n",
      "Epoch 133/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9764Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0749 - acc: 0.9767 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 134/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/90 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9955Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0136 - acc: 0.9956 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 135/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9909Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0430 - acc: 0.9911 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 136/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9943Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0195 - acc: 0.9933 - val_loss: 6.4586e-04 - val_acc: 1.0000\n",
      "Epoch 137/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9818Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0609 - acc: 0.9822 - val_loss: 0.0146 - val_acc: 0.9900\n",
      "Epoch 138/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9864Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0383 - acc: 0.9867 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 139/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9742Epoch 1/150\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 0.0956 - acc: 0.9722 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 140/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9798Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0761 - acc: 0.9800 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 141/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9798Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0563 - acc: 0.9789 - val_loss: 0.0110 - val_acc: 0.9900\n",
      "Epoch 142/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9750Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0771 - acc: 0.9756 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 143/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9886Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0321 - acc: 0.9889 - val_loss: 0.0248 - val_acc: 0.9900\n",
      "Epoch 144/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9875Epoch 1/150\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 0.0455 - acc: 0.9878 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 145/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9843Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0483 - acc: 0.9833 - val_loss: 0.0284 - val_acc: 0.9900\n",
      "Epoch 146/150\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9865Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0412 - acc: 0.9867 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 147/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9875Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0218 - acc: 0.9878 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 148/150\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9908Epoch 1/150\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 0.0345 - acc: 0.9911 - val_loss: 0.0327 - val_acc: 0.9900\n",
      "Epoch 149/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9852Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0651 - acc: 0.9856 - val_loss: 0.0437 - val_acc: 0.9900\n",
      "Epoch 150/150\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9920Epoch 1/150\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 0.0192 - acc: 0.9922 - val_loss: 0.0035 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2217402bfc8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()\n",
    "model.fit(train_gen,\n",
    "          epochs = 150,\n",
    "          validation_data = val_gen,\n",
    "          verbose=1\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "acc = model.history.history['acc']\n",
    "val_acc = model.history.history['val_acc']\n",
    "loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0035 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0035050890034001724, 1.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_gen[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 6.6061345e-26, 3.7261272e-21, 8.3644863e-26,\n",
       "        1.4701367e-18, 3.5100196e-25, 1.0000000e+00, 1.8000027e-29,\n",
       "        6.5100521e-24, 0.0000000e+00],\n",
       "       [7.7255656e-33, 3.2427876e-23, 2.7256715e-25, 5.9492033e-21,\n",
       "        1.7734075e-11, 6.0042270e-18, 7.0824490e-28, 5.7439351e-18,\n",
       "        1.0000000e+00, 9.6644425e-34]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "export_dir='model/'\n",
    "tf.saved_model.save(model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "\n",
    "mode=\"speed\"\n",
    "\n",
    "if mode==\"storage\":\n",
    "    opt = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
    "if mode==\"speed\":\n",
    "    opt = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
    "else :\n",
    "    opt = tf.lite.Optimize.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from model/variables\\variables\n",
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default', '__saved_model_init_op'}\n",
      "INFO:tensorflow:input tensors info: \n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: conv2d_15_input\n",
      "INFO:tensorflow: tensor name: serving_default_conv2d_15_input:0, shape: (-1, 50, 50, 1), type: DT_FLOAT\n",
      "INFO:tensorflow:output tensors info: \n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_11\n",
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 10), type: DT_FLOAT\n",
      "INFO:tensorflow:Restoring parameters from model/variables\\variables\n",
      "WARNING:tensorflow:From C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 18 variables.\n",
      "INFO:tensorflow:Converted 18 variables to const ops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function."
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "See console for info.\n2022-05-24 14:25:34.267363: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\n2022-05-24 14:25:34.267398: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-05-24 14:25:36.561667: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: IdentityN\n2022-05-24 14:25:36.565725: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 105 operators, 128 arrays (0 quantized)\n2022-05-24 14:25:36.567653: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 105 operators, 128 arrays (0 quantized)\n2022-05-24 14:25:36.576258: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 19 operators, 39 arrays (0 quantized)\n2022-05-24 14:25:36.578478: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 18 operators, 38 arrays (0 quantized)\n2022-05-24 14:25:36.578870: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 17 operators, 36 arrays (0 quantized)\n2022-05-24 14:25:36.579204: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 17 operators, 36 arrays (0 quantized)\n2022-05-24 14:25:36.579486: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 17 operators, 36 arrays (0 quantized)\n2022-05-24 14:25:36.582013: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 100032 bytes, theoretical optimal value: 100032 bytes.\n2022-05-24 14:25:36.582122: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 5606474 ops, equivalently 2803237 MACs\n2022-05-24 14:25:36.582130: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 762586\n2022-05-24 14:25:36.585505: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\nTraceback (most recent call last):\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\wijay\\anaconda3\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 312, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 258, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1644\\775734241.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtflite_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    981\u001b[0m           \u001b[0minput_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m           \u001b[0moutput_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m           **converter_kwargs)\n\u001b[0m\u001b[0;32m    984\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m       result = _toco_convert_graph_def(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[1;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdebug_info_str\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m       enable_mlir_converter=enable_mlir_converter)\n\u001b[0m\u001b[0;32m    450\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[1;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[0;32m    198\u001b[0m       \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m       \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mConverterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"See console for info.\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;31m# Must manually cleanup files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConverterError\u001b[0m: See console for info.\n2022-05-24 14:25:34.267363: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\n2022-05-24 14:25:34.267398: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-05-24 14:25:36.561667: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: IdentityN\n2022-05-24 14:25:36.565725: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 105 operators, 128 arrays (0 quantized)\n2022-05-24 14:25:36.567653: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 105 operators, 128 arrays (0 quantized)\n2022-05-24 14:25:36.576258: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 19 operators, 39 arrays (0 quantized)\n2022-05-24 14:25:36.578478: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 18 operators, 38 arrays (0 quantized)\n2022-05-24 14:25:36.578870: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 17 operators, 36 arrays (0 quantized)\n2022-05-24 14:25:36.579204: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 17 operators, 36 arrays (0 quantized)\n2022-05-24 14:25:36.579486: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 17 operators, 36 arrays (0 quantized)\n2022-05-24 14:25:36.582013: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 100032 bytes, theoretical optimal value: 100032 bytes.\n2022-05-24 14:25:36.582122: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 5606474 ops, equivalently 2803237 MACs\n2022-05-24 14:25:36.582130: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 762586\n2022-05-24 14:25:36.585505: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\nTraceback (most recent call last):\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\wijay\\anaconda3\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 312, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 258, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\Users\\wijay\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\n\n\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "\n",
    "converter.optimizations = [opt]\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SAI.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "9d14f3cb4f3f66916ff7792d598eca6bcb6161e9a0aec2e5c2bb64d090dc6dc0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
